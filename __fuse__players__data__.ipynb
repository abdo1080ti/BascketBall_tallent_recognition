{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f50fee9",
   "metadata": {},
   "source": [
    "### **fuse (combine) the 4 limbs csv files in one csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "849d5acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea21e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_base = r'C:\\uni\\goalkeeper_data'\n",
    "output_base = r'C:\\uni\\just_testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b34ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "limb_files = [\n",
    "    ('RightArm.csv', 'RA'),\n",
    "    ('LeftArm.csv', 'LA'),\n",
    "    ('RightLeg.csv', 'RL'),\n",
    "    ('LeftLeg.csv', 'LL')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a117ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_columns(df, prefix, skip_cols=['server_timestamp', 'session_id', 'goalkeeper_id', 'shot_result', 'type']):\n",
    "    return df.rename(columns={col: f\"{prefix}_{col}\" for col in df.columns if col not in skip_cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6ccd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused: C:\\uni\\just_testing\\goalkeeper_10\\session_1.csv\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_100\\session_1.csv\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_1000\\session_1.csv\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_1000\\session_2.csv\n",
      "Missing LeftArm.csv in C:\\uni\\goalkeeper_data\\goalkeeper_11\\session_4\n",
      "Skipping session due to missing limbs: C:\\uni\\goalkeeper_data\\goalkeeper_11\\session_4\n",
      "Missing LeftArm.csv in C:\\uni\\goalkeeper_data\\goalkeeper_15\\session_4\n",
      "Skipping session due to missing limbs: C:\\uni\\goalkeeper_data\\goalkeeper_15\\session_4\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_16\\session_4.csv\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_20\\session_1.csv\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_201\\session_1.csv\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_203\\session_1.csv\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_204\\session_1.csv\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_205\\session_1.csv\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_22\\session_1.csv\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_30\\session_1.csv\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_31\\session_1.csv\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_55\\session_1.csv\n",
      "Missing RightArm.csv in C:\\uni\\goalkeeper_data\\goalkeeper_6\\session_5\n",
      "Skipping session due to missing limbs: C:\\uni\\goalkeeper_data\\goalkeeper_6\\session_5\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_6\\session_f.csv\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_6\\session_h.csv\n",
      "Missing LeftArm.csv in C:\\uni\\goalkeeper_data\\goalkeeper_7\\session_1\n",
      "Skipping session due to missing limbs: C:\\uni\\goalkeeper_data\\goalkeeper_7\\session_1\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_7\\session_2.csv\n",
      "Missing RightArm.csv in C:\\uni\\goalkeeper_data\\goalkeeper_7\\session_7\n",
      "Skipping session due to missing limbs: C:\\uni\\goalkeeper_data\\goalkeeper_7\\session_7\n",
      "Fused: C:\\uni\\just_testing\\goalkeeper_9\\session_5.csv\n"
     ]
    }
   ],
   "source": [
    "for player in os.listdir(input_base):\n",
    "    player_path = os.path.join(input_base, player)\n",
    "    if not os.path.isdir(player_path): continue\n",
    "\n",
    "    for session in os.listdir(player_path):\n",
    "        session_path = os.path.join(player_path, session)\n",
    "        if not os.path.isdir(session_path): continue\n",
    "\n",
    "        dfs = {}\n",
    "        lengths = []\n",
    "\n",
    "        for filename, prefix in limb_files:\n",
    "            path = os.path.join(session_path, filename)\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Missing {filename} in {session_path}\")\n",
    "                break\n",
    "            df = pd.read_csv(path)\n",
    "            dfs[prefix] = df\n",
    "            lengths.append(len(df))\n",
    "\n",
    "        if len(dfs) != 4:\n",
    "            print(f\"Skipping session due to missing limbs: {session_path}\")\n",
    "            continue\n",
    "\n",
    "        min_len = min(lengths)\n",
    "\n",
    "        shortest_idx = lengths.index(min_len)\n",
    "        timestamp_source_prefix = limb_files[shortest_idx][1]\n",
    "        server_timestamp = dfs[timestamp_source_prefix]['server_timestamp'].iloc[:min_len].reset_index(drop=True)\n",
    "\n",
    "        base_cols = ['session_id', 'goalkeeper_id', 'shot_result', 'type']\n",
    "        base_info = dfs[timestamp_source_prefix][base_cols].iloc[:min_len].reset_index(drop=True)\n",
    "\n",
    "        fused_signals = []\n",
    "        for prefix, df in dfs.items():\n",
    "            sensor_data = df.drop(columns=['server_timestamp', *base_cols]).iloc[:min_len].reset_index(drop=True)\n",
    "            sensor_data = prefix_columns(sensor_data, prefix)\n",
    "            fused_signals.append(sensor_data)\n",
    "\n",
    "        fused_df = pd.concat([server_timestamp, base_info] + fused_signals, axis=1)\n",
    "\n",
    "        out_dir = os.path.join(output_base, player)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        out_path = os.path.join(out_dir, f\"{session}.csv\")\n",
    "        fused_df.to_csv(out_path, index=False)\n",
    "        print(f\"Fused: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73e7bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMBINING ALL FUSED SESSION FILES INTO ONE DATASET\n",
      "============================================================\n",
      "Processing player: goalkeeper_10\n",
      "  Added: session_1.csv (1162 rows)\n",
      "Processing player: goalkeeper_100\n",
      "  Added: session_1.csv (241 rows)\n",
      "Processing player: goalkeeper_1000\n",
      "  Added: session_1.csv (1016 rows)\n",
      "  Added: session_2.csv (779 rows)\n",
      "Processing player: goalkeeper_16\n",
      "  Added: session_4.csv (198 rows)\n",
      "Processing player: goalkeeper_20\n",
      "  Added: session_1.csv (358 rows)\n",
      "Processing player: goalkeeper_201\n",
      "  Added: session_1.csv (8 rows)\n",
      "Processing player: goalkeeper_203\n",
      "  Added: session_1.csv (455 rows)\n",
      "Processing player: goalkeeper_204\n",
      "  Added: session_1.csv (2353 rows)\n",
      "Processing player: goalkeeper_205\n",
      "  Added: session_1.csv (558 rows)\n",
      "Processing player: goalkeeper_22\n",
      "  Added: session_1.csv (572 rows)\n",
      "Processing player: goalkeeper_30\n",
      "  Added: session_1.csv (83 rows)\n",
      "Processing player: goalkeeper_31\n",
      "  Added: session_1.csv (272 rows)\n",
      "Processing player: goalkeeper_55\n",
      "  Added: session_1.csv (189 rows)\n",
      "Processing player: goalkeeper_6\n",
      "  Added: session_f.csv (153 rows)\n",
      "  Added: session_h.csv (550 rows)\n",
      "Processing player: goalkeeper_7\n",
      "  Added: session_2.csv (559 rows)\n",
      "Processing player: goalkeeper_9\n",
      "  Added: session_5.csv (400 rows)\n",
      "\n",
      "Combining 18 session files...\n",
      "✅ Successfully created: C:\\uni\\just_testing\\fused_data.csv\n",
      "Total rows: 9,906\n",
      "Total columns: 73\n",
      "\n",
      "First few rows of combined data:\n",
      "             server_timestamp session_id  goalkeeper_id shot_result  \\\n",
      "0  2025-05-31T02:43:31.695861          1             10     unknown   \n",
      "1  2025-05-31T02:43:31.697114          1             10     unknown   \n",
      "2  2025-05-31T02:43:31.698533          1             10     unknown   \n",
      "3  2025-05-31T02:43:31.783884          1             10     unknown   \n",
      "4  2025-05-31T02:43:31.841937          1             10     unknown   \n",
      "\n",
      "             type RA_limb_id RA_limb_name  RA_device_timestamp  RA_accel_x  \\\n",
      "0  sensor_reading         RA     RightArm                79689       -9.69   \n",
      "1  sensor_reading         RA     RightArm                79779       -9.69   \n",
      "2  sensor_reading         RA     RightArm                79869       -9.69   \n",
      "3  sensor_reading         RA     RightArm                79959       -9.68   \n",
      "4  sensor_reading         RA     RightArm                80049       -9.71   \n",
      "\n",
      "   RA_accel_y  ...  RA_coach_feedback  LA_device_id  LA_device_name  \\\n",
      "0       -0.53  ...                NaN           NaN             NaN   \n",
      "1       -0.53  ...                NaN           NaN             NaN   \n",
      "2       -0.55  ...                NaN           NaN             NaN   \n",
      "3       -0.53  ...                NaN           NaN             NaN   \n",
      "4       -0.51  ...                NaN           NaN             NaN   \n",
      "\n",
      "   LA_coach_feedback  RL_device_id  RL_device_name  RL_coach_feedback  \\\n",
      "0                NaN           NaN             NaN                NaN   \n",
      "1                NaN           NaN             NaN                NaN   \n",
      "2                NaN           NaN             NaN                NaN   \n",
      "3                NaN           NaN             NaN                NaN   \n",
      "4                NaN           NaN             NaN                NaN   \n",
      "\n",
      "   LL_device_id  LL_device_name LL_coach_feedback  \n",
      "0           NaN             NaN               NaN  \n",
      "1           NaN             NaN               NaN  \n",
      "2           NaN             NaN               NaN  \n",
      "3           NaN             NaN               NaN  \n",
      "4           NaN             NaN               NaN  \n",
      "\n",
      "[5 rows x 73 columns]\n",
      "\n",
      "Columns in combined data:\n",
      "['server_timestamp', 'session_id', 'goalkeeper_id', 'shot_result', 'type', 'RA_limb_id', 'RA_limb_name', 'RA_device_timestamp', 'RA_accel_x', 'RA_accel_y', 'RA_accel_z', 'RA_gyro_x', 'RA_gyro_y', 'RA_gyro_z', 'RA_mag_x', 'RA_mag_y', 'RA_mag_z', 'RA_pressure', 'RA_temperature', 'LA_limb_id', 'LA_limb_name', 'LA_device_timestamp', 'LA_accel_x', 'LA_accel_y', 'LA_accel_z', 'LA_gyro_x', 'LA_gyro_y', 'LA_gyro_z', 'LA_mag_x', 'LA_mag_y', 'LA_mag_z', 'LA_pressure', 'LA_temperature', 'RL_limb_id', 'RL_limb_name', 'RL_device_timestamp', 'RL_accel_x', 'RL_accel_y', 'RL_accel_z', 'RL_gyro_x', 'RL_gyro_y', 'RL_gyro_z', 'RL_mag_x', 'RL_mag_y', 'RL_mag_z', 'RL_pressure', 'RL_temperature', 'LL_limb_id', 'LL_limb_name', 'LL_device_timestamp', 'LL_accel_x', 'LL_accel_y', 'LL_accel_z', 'LL_gyro_x', 'LL_gyro_y', 'LL_gyro_z', 'LL_mag_x', 'LL_mag_y', 'LL_mag_z', 'LL_pressure', 'LL_temperature', 'RA_device_id', 'RA_device_name', 'RA_coach_feedback', 'LA_device_id', 'LA_device_name', 'LA_coach_feedback', 'RL_device_id', 'RL_device_name', 'RL_coach_feedback', 'LL_device_id', 'LL_device_name', 'LL_coach_feedback']\n"
     ]
    }
   ],
   "source": [
    "# =================== COMBINE ALL FUSED FILES INTO ONE ===================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "output_base = r'C:\\uni\\just_testing'\n",
    "\n",
    "all_fused_dfs = []\n",
    "session_count = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMBINING ALL FUSED SESSION FILES INTO ONE DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if output directory exists\n",
    "if not os.path.exists(output_base):\n",
    "    print(f\"Output directory not found: {output_base}\")\n",
    "    print(\"Make sure the first part ran successfully!\")\n",
    "else:\n",
    "    # Loop through all player folders\n",
    "    for player in os.listdir(output_base):\n",
    "        player_path = os.path.join(output_base, player)\n",
    "        \n",
    "        # Skip if not a directory\n",
    "        if not os.path.isdir(player_path):\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing player: {player}\")\n",
    "        \n",
    "        # Loop through all session files in player folder\n",
    "        for session_file in os.listdir(player_path):\n",
    "            if session_file.endswith('.csv'):\n",
    "                session_path = os.path.join(player_path, session_file)\n",
    "                \n",
    "                try:\n",
    "                    # Read the session CSV\n",
    "                    df = pd.read_csv(session_path)\n",
    "                    all_fused_dfs.append(df)\n",
    "                    session_count += 1\n",
    "                    print(f\"  Added: {session_file} ({len(df)} rows)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error reading {session_file}: {e}\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if all_fused_dfs:\n",
    "        print(f\"\\nCombining {session_count} session files...\")\n",
    "        combined_df = pd.concat(all_fused_dfs, ignore_index=True)\n",
    "        \n",
    "        # Save the combined file\n",
    "        combined_path = os.path.join(output_base, 'fused_data.csv')\n",
    "        combined_df.to_csv(combined_path, index=False)\n",
    "        \n",
    "        print(f\"✅ Successfully created: {combined_path}\")\n",
    "        print(f\"Total rows: {len(combined_df):,}\")\n",
    "        print(f\"Total columns: {len(combined_df.columns)}\")\n",
    "        \n",
    "        # Show sample of the combined data\n",
    "        print(\"\\nFirst few rows of combined data:\")\n",
    "        print(combined_df.head())\n",
    "        print(\"\\nColumns in combined data:\")\n",
    "        print(list(combined_df.columns))\n",
    "    else:\n",
    "        print(\"❌ No fused session files found!\")\n",
    "        print(\"Make sure:\")\n",
    "        print(\"1. Your input directory has the limb CSV files\")\n",
    "        print(\"2. The first part of the code ran successfully\")\n",
    "        print(\"3. Check the folder structure:\")\n",
    "        print(f\"   {output_base}\\\\player_name\\\\session_name.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c89d42",
   "metadata": {},
   "source": [
    "### **add the labels (coach feedback) to the players fused data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5daf9755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdo1\\AppData\\Local\\Temp\\ipykernel_21232\\1622397438.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  motion_df = pd.read_csv(r'C:\\uni\\just_testing\\fused_data.csv')\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\uni\\\\just_testing\\\\coach_feedback.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m motion_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124muni\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mjust_testing\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfused_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m feedback_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124muni\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mjust_testing\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcoach_feedback.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\uni\\\\just_testing\\\\coach_feedback.csv'"
     ]
    }
   ],
   "source": [
    "motion_df = pd.read_csv(r'C:\\uni\\just_testing\\fused_data.csv')\n",
    "feedback_df = pd.read_csv(r'C:\\uni\\just_testing\\coach_feedback.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45fdad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'motion_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m motion_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp_sec\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(motion_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserver_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mfloor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m feedback_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp_sec\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(feedback_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserver_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mfloor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'motion_df' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "motion_df['timestamp_sec'] = pd.to_datetime(motion_df['server_timestamp']).dt.floor('s')\n",
    "feedback_df['timestamp_sec'] = pd.to_datetime(feedback_df['server_timestamp']).dt.floor('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_df['coach_feedback'] = 'undefined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b00c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, feedback_row in feedback_df.iterrows():\n",
    "    session = feedback_row['session_id']\n",
    "    keeper = feedback_row['goalkeeper_id']\n",
    "    feedback_time = feedback_row['timestamp_sec']\n",
    "    feedback_value = feedback_row['coach_feedback']\n",
    "\n",
    "    # define the (+-) 2 second time window\n",
    "    start_time = feedback_time - pd.Timedelta(seconds=2)\n",
    "    end_time = feedback_time + pd.Timedelta(seconds=2)\n",
    "\n",
    "    condition = (\n",
    "        (motion_df['session_id'] == session) &\n",
    "        (motion_df['goalkeeper_id'] == keeper) &\n",
    "        (motion_df['timestamp_sec'] >= start_time) &\n",
    "        (motion_df['timestamp_sec'] <= end_time)\n",
    "    )\n",
    "\n",
    "    motion_df.loc[condition, 'coach_feedback'] = feedback_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21fc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_df.drop(columns='timestamp_sec', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb82715",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_df.to_csv('fused_labeled_data.csv', index=False)\n",
    "print('dataset saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b662333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
